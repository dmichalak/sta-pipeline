{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import starfile\n",
    "\n",
    "\"\"\"\n",
    "Goal = find and remove bad particles in peak_local_max output\n",
    "\n",
    "Requires = a unique particle identifier = particle coordinates and ts_name\n",
    "\n",
    "\n",
    "Using run_it025_data.star...\n",
    "\n",
    "Use = \"rlnCoordinateX\", \"rlnCoordinateY\", \"rlnCoordinateZ\", \"rlnTomoName\"\n",
    "\n",
    "Refinement stores translation and rotation info in the following columns:\n",
    "    rlnOriginXAngst\n",
    "    rlnOriginYAngst\n",
    "    rlnOriginZAngst\n",
    "    rlnAngleRot\n",
    "    rlnAngleTilt\n",
    "    rlnAnglePsi\n",
    "\n",
    "Classification results are in the following columns:\n",
    "    rlnClassNumber\n",
    "    rlnLogLikeliContribution\n",
    "    rlnMaxValueProbDistribution\n",
    "    rlnNrOfSignificantSamples - not a good indicator of quality for cleaning\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = Path(\"/mnt/scratch/ribosomes/wws_EGFcontrol/\")\n",
    "\n",
    "peaks_star_path = project_dir / Path(\"subset_abs5rel0.1.star\")\n",
    "peaks_df = starfile.read(peaks_star_path)\n",
    "\n",
    "class_data_star_path = project_dir / Path(\"Class3D/init_bin4/run_it025_data.star\")\n",
    "class_df = starfile.read(class_data_star_path)\n",
    "class_particles_df = class_df[\"particles\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input bad classes and separate them from good classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of particles in each class:\n",
      "rlnClassNumber\n",
      "2    1151\n",
      "3       8\n",
      "Name: count, dtype: int64\n",
      "Number of particles in the data after removing bad classes:\n",
      "1151\n",
      "Number of bad particles:\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "bad_classes = [\"3\"]\n",
    "\n",
    "bad_peaks_star_path = project_dir / Path(\"bad_peaks.star\")\n",
    "# bad_particles.star will be saved in the same directory as class_data_star_path\n",
    "bad_particles_star_path = class_data_star_path.parent / Path(\"bad_particles.star\")\n",
    "cleaned_data_star_path = class_data_star_path.parent / Path(\"cleaned_run_it025_data.star\")\n",
    "\n",
    "# Print the number of particles in each class\n",
    "print(\"Number of particles in each class:\")\n",
    "print(class_particles_df[\"rlnClassNumber\"].value_counts())\n",
    "\n",
    "# Remove bad classes from the class_particles_df and save to a new df\n",
    "cleaned_particles_df = class_particles_df[~class_particles_df[\"rlnClassNumber\"].astype(str).isin(bad_classes)]\n",
    "bad_particles_df = class_particles_df[class_particles_df[\"rlnClassNumber\"].astype(str).isin(bad_classes)]\n",
    "# Print the number of particles in the cleaned data\n",
    "print(\"Number of particles in the data after removing bad classes:\")\n",
    "print(cleaned_particles_df.shape[0])\n",
    "# Print the number of bad particles\n",
    "print(\"Number of bad particles:\")\n",
    "print(bad_particles_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Save the bad particles to a star file. If the file already exists, append to it if the particles are not already in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of new bad particles in this classification:\n",
      "8\n",
      "Number of previously saved bad peaks:\n",
      "8\n",
      "Number of new bad particles:\n",
      "8\n",
      "Number of bad peaks after the last classification:\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "if bad_particles_star_path.exists():\n",
    "    bad_particles_star_df = starfile.read(bad_particles_star_path)\n",
    "    # Print the number of new bad particles\n",
    "    print(\"Number of new bad particles in this classification:\")\n",
    "    print(bad_particles_df.shape[0])\n",
    "    # Save the bad particles to a star file\n",
    "    starfile.write(bad_particles_star_df, bad_particles_star_path, overwrite=True)\n",
    "else:\n",
    "    starfile.write(bad_particles_df, bad_particles_star_path, overwrite=False)\n",
    "\n",
    "# Save the cleaned data to a star file\n",
    "class_df[\"particles\"] = cleaned_particles_df\n",
    "starfile.write(class_df, cleaned_data_star_path, overwrite=True)\n",
    "\n",
    "# Append the bad particles to bad_peaks_star_path\n",
    "if bad_peaks_star_path.exists():\n",
    "    bad_peaks_star_df = starfile.read(bad_peaks_star_path)\n",
    "    # Print the number of bad particles in the previous classification\n",
    "    print(\"Number of previously saved bad peaks:\")\n",
    "    print(bad_peaks_star_df.shape[0])\n",
    "    # Print the number of new bad particles\n",
    "    print(\"Number of new bad particles:\")\n",
    "    print(bad_particles_df.shape[0])\n",
    "    # Append the new bad particles to the old ones\n",
    "    bad_peaks_star_df = pd.concat([bad_peaks_star_df, bad_particles_df], ignore_index=True)\n",
    "    # Drop duplicates\n",
    "    bad_peaks_star_df = bad_peaks_star_df.drop_duplicates(subset=[\"rlnCoordinateX\", \"rlnCoordinateY\", \"rlnCoordinateZ\", \"rlnTomoName\"], keep=\"first\")\n",
    "    # Print the number of bad particles after the last classification\n",
    "    print(\"Number of bad peaks after the last classification:\")\n",
    "    print(bad_peaks_star_df.shape[0])\n",
    "    # Save the bad particles to a star file\n",
    "    starfile.write(bad_peaks_star_df, bad_peaks_star_path, overwrite=True)\n",
    "else:\n",
    "    starfile.write(bad_particles_df, bad_peaks_star_path, overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Create a new particle STAR file from the new peaks file by removing previously processed peaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of peaks in the processed data:\n",
      "1159\n",
      "Number of peaks in the new file:\n",
      "4601\n",
      "Number of peaks after appending:\n",
      "5760\n",
      "Number of peaks after dropping duplicates:\n",
      "4601\n",
      "This should be equal to the number of peaks in the new file.\n"
     ]
    }
   ],
   "source": [
    "# First, read the previously processed peaks\n",
    "processed_star_path = project_dir / Path(\"Class3D/init_bin4/run_it025_data.star\")\n",
    "processed_star_df = starfile.read(processed_star_path)\n",
    "new_star_df = processed_star_df.copy()\n",
    "# Print the number of peaks in the processed data\n",
    "print(\"Number of peaks in the processed data:\")\n",
    "print(processed_star_df[\"particles\"].shape[0])\n",
    "# Then, read the new peaks\n",
    "new_peaks_star_path = project_dir / Path(\"particles_abs5rel0.1.star\")\n",
    "new_peaks_star_df = starfile.read(new_peaks_star_path)\n",
    "# Print the number of new peaks\n",
    "print(\"Number of peaks in the new file:\")\n",
    "print(new_peaks_star_df.shape[0])\n",
    "# Append the new peaks to the old ones\n",
    "new_particles_df = pd.concat([processed_star_df[\"particles\"], new_peaks_star_df], ignore_index=True)\n",
    "# Print the number of peaks after appending\n",
    "print(\"Number of peaks after appending:\")\n",
    "print(new_particles_df.shape[0])\n",
    "# Drop duplicates\n",
    "new_particles_df = new_particles_df.drop_duplicates(subset=[\"rlnCoordinateX\", \"rlnCoordinateY\", \"rlnCoordinateZ\", \"rlnTomoName\"], keep=\"first\")\n",
    "# Print the number of peaks after dropping duplicates\n",
    "print(\"Number of peaks after dropping duplicates:\")\n",
    "print(new_particles_df.shape[0])\n",
    "print(\"This should be equal to the number of peaks in the new file.\")\n",
    "# Save the new star file\n",
    "new_particles_df.drop(columns=[\n",
    "    \"rlnRandomSubset\",\n",
    "    \"rlnGroupNumber\", \n",
    "    \"rlnClassNumber\", \n",
    "    \"rlnNormCorrection\", \n",
    "    \"rlnLogLikeliContribution\", \n",
    "    \"rlnMaxValueProbDistribution\", \n",
    "    \"rlnNrOfSignificantSamples\",\n",
    "    #\"rlnOpticsGroup\",\n",
    "    \"rlnImageName\",\n",
    "    \"rlnCtfImage\",\n",
    "    \"staParticleIndex\",\n",
    "    \n",
    "    ], axis=1, inplace=True)\n",
    "new_particles_df[\"rlnOpticsGroup\"] = 1\n",
    "new_particles_df[[\"rlnOriginXAngst\", \"rlnOriginYAngst\", \"rlnOriginZAngst\"]] = new_particles_df[[\"rlnOriginXAngst\", \"rlnOriginYAngst\", \"rlnOriginZAngst\"]].fillna(0)\n",
    "new_star_df[\"particles\"] = new_particles_df\n",
    "new_star_df[\"optics\"].drop(columns=[\n",
    "    \"rlnCtfDataAreCtfPremultiplied\",\n",
    "    \"rlnImageDimensionality\",\n",
    "    \"rlnTomoSubtomogramBinning\",\n",
    "    \"rlnImagePixelSize\",\n",
    "    \"rlnImageSize\",\n",
    "], axis=1, inplace=True)\n",
    "# Save the new star file\n",
    "new_star_path = project_dir / Path(f\"ready_{new_peaks_star_path.stem}.star\")\n",
    "starfile.write(new_star_df, new_star_path, overwrite=True)\n",
    "\n",
    "# Save the new star file to the optimisation set\n",
    "# This is a temporary solution until I figure out how to do this in the pipeline\n",
    "# IT MIGHT NOT WORK \n",
    "old_optimisation_set = Path(\"/mnt/scratch/ribosomes/wws_EGFcontrol/ImportTomo/job002/optimisation_set.star\")\n",
    "old_optimisation_set_df = starfile.read(old_optimisation_set)\n",
    "new_optimisation_set_df = old_optimisation_set_df.copy()\n",
    "new_optimisation_set_df[\"rlnTomoParticlesFile\"] = new_star_path.name\n",
    "starfile.write(new_optimisation_set_df, project_dir / Path(\"new_optimisation_set.star\"), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
