{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGoal = find and remove bad particles in peak_local_max output\\n\\nRequires = a unique particle identifier = particle coordinates and ts_name\\n\\n\\nUsing run_it025_data.star...\\n\\nUse = \"rlnCoordinateX\", \"rlnCoordinateY\", \"rlnCoordinateZ\", \"rlnTomoName\"\\n\\nRefinement stores translation and rotation info in the following columns:\\n    rlnOriginXAngst\\n    rlnOriginYAngst\\n    rlnOriginZAngst\\n    rlnAngleRot\\n    rlnAngleTilt\\n    rlnAnglePsi\\n\\nClassification results are in the following columns:\\n    rlnClassNumber\\n    rlnLogLikeliContribution\\n    rlnMaxValueProbDistribution\\n    rlnNrOfSignificantSamples - not a good indicator of quality for cleaning\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import starfile\n",
    "\n",
    "\"\"\"\n",
    "Goal = find and remove bad particles in peak_local_max output\n",
    "\n",
    "Requires = a unique particle identifier = particle coordinates and ts_name\n",
    "\n",
    "\n",
    "Using run_it025_data.star...\n",
    "\n",
    "Use = \"rlnCoordinateX\", \"rlnCoordinateY\", \"rlnCoordinateZ\", \"rlnTomoName\"\n",
    "\n",
    "Refinement stores translation and rotation info in the following columns:\n",
    "    rlnOriginXAngst\n",
    "    rlnOriginYAngst\n",
    "    rlnOriginZAngst\n",
    "    rlnAngleRot\n",
    "    rlnAngleTilt\n",
    "    rlnAnglePsi\n",
    "\n",
    "Classification results are in the following columns:\n",
    "    rlnClassNumber\n",
    "    rlnLogLikeliContribution\n",
    "    rlnMaxValueProbDistribution\n",
    "    rlnNrOfSignificantSamples - not a good indicator of quality for cleaning\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set paths and junk classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = Path(\"/mnt/scratch/ribosomes/wws_EGFcontrol/\")\n",
    "\n",
    "processed_peaks_star_path = project_dir / Path(\"subset_abs5rel0.1.star\")\n",
    "processed_classes_path = project_dir / Path(\"Class3D/init_bin4/run_it025_data.star\")\n",
    "\n",
    "junk_classes = [\"3\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate particles into junk and usable groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of particles in each class:\n",
      "rlnClassNumber\n",
      "2    1151\n",
      "3       8\n",
      "Name: count, dtype: int64\n",
      "Number of particles in the data after removing bad classes:\n",
      "1151\n",
      "Number of bad particles:\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "processed_peaks_df = starfile.read(processed_peaks_star_path)\n",
    "processed_classes_df = starfile.read(processed_classes_path)\n",
    "processed_classes_particles_df = processed_classes_df[\"particles\"]\n",
    "\n",
    "identity_columns = [\"rlnCoordinateX\", \"rlnCoordinateY\", \"rlnCoordinateZ\", \"rlnTomoName\"]\n",
    "# bad_particles.star will be saved in the same directory as classified_data_star_path\n",
    "\n",
    "# Print the number of particles in each class\n",
    "print(\"Number of particles in each class:\")\n",
    "print(processed_classes_particles_df[\"rlnClassNumber\"].value_counts())\n",
    "\n",
    "# Remove bad classes from the classified_particles_df and save to a new df\n",
    "processed_usable_df = processed_classes_particles_df[~processed_classes_particles_df[\"rlnClassNumber\"].astype(str).isin(junk_classes)]\n",
    "processed_junk_df= processed_classes_particles_df[\n",
    "    processed_classes_particles_df[\"rlnClassNumber\"].astype(str).isin(junk_classes)\n",
    "    ]\n",
    "# Print the number of particles in the cleaned data\n",
    "print(\"Number of particles in the data after removing bad classes:\")\n",
    "print(processed_usable_df.shape[0])\n",
    "# Print the number of bad particles\n",
    "print(\"Number of bad particles:\")\n",
    "print(processed_junk_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Save the groups to star files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of previously saved bad peaks:\n",
      "8\n",
      "Number of new bad particles:\n",
      "8\n",
      "Number of total junk peaks after the last classification:\n",
      "8\n",
      "Number of previously saved usable particles:\n",
      "1151\n",
      "Number of new usable particles:\n",
      "1151\n",
      "Number of total usable particles after the last classification:\n",
      "1151\n"
     ]
    }
   ],
   "source": [
    "# Save the junk found in this classification\n",
    "processed_junk_path= processed_classes_path.parent / Path(\"junk_run_it025_data.star\")\n",
    "starfile.write(processed_junk_df, processed_junk_path, overwrite=True)\n",
    "\n",
    "# Save the cleaned data to a star file\n",
    "processed_usable_path= processed_classes_path.parent / Path(\"usable_run_it025_data.star\")\n",
    "processed_classes_df[\"particles\"] = processed_usable_df\n",
    "starfile.write(processed_classes_df, processed_usable_path, overwrite=True)\n",
    "\n",
    "# Save the junk to the master list\n",
    "all_junk_path = project_dir / Path(\"all_junk.star\")\n",
    "if all_junk_path.exists():\n",
    "    # Append the new junk to the master list\n",
    "    all_junk_df = starfile.read(all_junk_path)\n",
    "    # Print the number of bad particles in the previous classification\n",
    "    print(\"Number of previously saved bad peaks:\")\n",
    "    print(all_junk_df.shape[0])\n",
    "    # Print the number of new bad particles\n",
    "    print(\"Number of new bad particles:\")\n",
    "    print(processed_junk_df.shape[0])\n",
    "    # Append the new bad particles to the old ones\n",
    "    all_junk_df = pd.concat([all_junk_df, processed_junk_df], ignore_index=True)\n",
    "    # Drop duplicates\n",
    "    all_junk_df = all_junk_df.drop_duplicates(subset=identity_columns, keep=\"first\")\n",
    "    # Print the number of bad particles after the last classification\n",
    "    print(\"Number of total junk peaks after the last classification:\")\n",
    "    print(all_junk_df.shape[0])\n",
    "    # Save the bad particles to a star file\n",
    "    starfile.write(all_junk_df, all_junk_path, overwrite=True)\n",
    "else:\n",
    "    starfile.write(processed_junk_df, all_junk_path, overwrite=False)\n",
    "\n",
    "# Save the usable to the master list\n",
    "all_usable_path = project_dir / Path(\"all_usable.star\")\n",
    "if all_usable_path.exists():\n",
    "    # Append the new usable to the master list\n",
    "    all_usable_df = starfile.read(all_usable_path)\n",
    "    # Print the number of usable particles in the previous classification\n",
    "    print(\"Number of previously saved usable particles:\")\n",
    "    print(all_usable_df.shape[0])\n",
    "    # Print the number of new usable particles\n",
    "    print(\"Number of new usable particles:\")\n",
    "    print(processed_usable_df.shape[0])\n",
    "    # Append the new usable particles to the old ones\n",
    "    all_usable_df = pd.concat([all_usable_df, processed_usable_df], ignore_index=True)\n",
    "    # Drop duplicates\n",
    "    all_usable_df = all_usable_df.drop_duplicates(subset=identity_columns, keep=\"first\")\n",
    "    # Print the number of usable particles after the last classification\n",
    "    print(\"Number of total usable particles after the last classification:\")\n",
    "    print(all_usable_df.shape[0])\n",
    "    # Save the usable particles to a star file\n",
    "    starfile.write(all_usable_df, all_usable_path, overwrite=True)\n",
    "else:\n",
    "    starfile.write(processed_usable_df, all_usable_path, overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Create a new peaks file with the junk particles removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of peaks in the processed data:\n",
      "1159\n",
      "Number of peaks in the new file:\n",
      "15397\n",
      "Number of peaks after appending:\n",
      "16556\n",
      "Number of peaks after dropping duplicates:\n",
      "15397\n",
      "This should be equal to the number of peaks in the new file.\n"
     ]
    }
   ],
   "source": [
    "# First, read the previously processed peaks\n",
    "processed_classes_path = project_dir / Path(\"Class3D/init_bin4/run_it025_data.star\")\n",
    "processed_classes_df = starfile.read(processed_classes_path)\n",
    "# Print the number of peaks in the processed data\n",
    "print(\"Number of peaks in the processed data:\")\n",
    "print(processed_classes_df[\"particles\"].shape[0])\n",
    "# Then, read the new peaks\n",
    "new_peaks_path = project_dir / Path(\"eman2/segmentations/particles_abs1rel0.1.star\")\n",
    "new_peaks_df = starfile.read(new_peaks_path)\n",
    "# Print the number of new peaks\n",
    "print(\"Number of peaks in the new file:\")\n",
    "print(new_peaks_df.shape[0])\n",
    "\n",
    "# Append the new peaks to the old ones\n",
    "new_peaks_df = pd.concat([processed_classes_df[\"particles\"], new_peaks_df], ignore_index=True)\n",
    "# Print the number of peaks after appending\n",
    "print(\"Number of peaks after appending:\")\n",
    "print(new_peaks_df.shape[0])\n",
    "# Drop duplicates\n",
    "new_peaks_df = new_peaks_df.drop_duplicates(subset=identity_columns, keep=\"first\")\n",
    "# Print the number of peaks after dropping duplicates\n",
    "print(\"Number of peaks after dropping duplicates:\")\n",
    "print(new_peaks_df.shape[0])\n",
    "print(\"This should be equal to the number of peaks in the new file.\")\n",
    "# Save the new star file\n",
    "new_peaks_df.drop(columns=[\n",
    "    \"rlnRandomSubset\",\n",
    "    \"rlnGroupNumber\", \n",
    "    \"rlnClassNumber\", \n",
    "    \"rlnNormCorrection\", \n",
    "    \"rlnLogLikeliContribution\", \n",
    "    \"rlnMaxValueProbDistribution\", \n",
    "    \"rlnNrOfSignificantSamples\",\n",
    "    \"rlnOpticsGroup\",\n",
    "    \"rlnImageName\",\n",
    "    \"rlnCtfImage\",\n",
    "#    \"staParticleIndex\",\n",
    "    \n",
    "    ], axis=1, inplace=True)\n",
    "#new_particles_df[\"rlnOpticsGroup\"] = 1\n",
    "#new_particles_df[[\"rlnOriginXAngst\", \"rlnOriginYAngst\", \"rlnOriginZAngst\"]] = new_particles_df[[\"rlnOriginXAngst\", \"rlnOriginYAngst\", \"rlnOriginZAngst\"]].fillna(0)\n",
    "#new_star_df[\"particles\"] = new_particles_df\n",
    "#new_star_df[\"optics\"].drop(columns=[\n",
    "#    \"rlnCtfDataAreCtfPremultiplied\",\n",
    "#    \"rlnImageDimensionality\",\n",
    "#    \"rlnTomoSubtomogramBinning\",\n",
    "#    \"rlnImagePixelSize\",\n",
    "#    \"rlnImageSize\",\n",
    "#], axis=1, inplace=True)\n",
    "# Save the new star file\n",
    "\n",
    "new_star_path = project_dir / Path(f\"ready_{new_peaks_path.stem}.star\")\n",
    "starfile.write(new_peaks_df, new_star_path, overwrite=True)\n",
    "\n",
    "## Replace rlnOriginXAngst, rlnOriginYAngst, rlnOriginZAngst with the previously found values\n",
    "## Read the newly imported particles\n",
    "#imported_particles_star_path = project_dir / Path(\"ImportTomo/abs5rel0.1/particles.star\")\n",
    "#imported_particles_star_df = starfile.read(imported_particles_star_path)\n",
    "## Read the previously processed particles\n",
    "#processed_particles_star_path = project_dir / Path(\"ready_particles_abs5rel0.1.star\")\n",
    "#processed_particles_star_df = starfile.read(processed_particles_star_path)\n",
    "#\n",
    "## If a row in imported_particles_star_df has the same identity_columns as a row in processed_particles_star_df, replace the [\"rlnOriginXAngst\", \"rlnOriginYAngst\", \"rlnOriginZAngst\"] values in that row in imported_particles_star_df with the values in the row in processed_particles_star_df\n",
    "#for index_processed, row_processed in processed_particles_star_df[\"particles\"].iterrows():\n",
    "#    for index_imported, row_imported in imported_particles_star_df[\"particles\"].iterrows():\n",
    "#        if index_imported % 1000 == 0 and index_imported != 0:\n",
    "#            print(f\"Processed {index_imported} rows.\")\n",
    "#        if (row_processed[identity_columns] == row_imported[identity_columns]).all():\n",
    "#            imported_particles_star_df[\"particles\"].loc[index_imported, [\"rlnOriginXAngst\", \"rlnOriginYAngst\", \"rlnOriginZAngst\"]] = row_processed[[\"rlnOriginXAngst\", \"rlnOriginYAngst\", \"rlnOriginZAngst\"]]\n",
    "#\n",
    "## Save the new star file\n",
    "#save_path = project_dir / Path(f\"ready_{imported_particles_star_path.stem}.star\")\n",
    "#starfile.write(imported_particles_star_df, save_path, overwrite=True)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
